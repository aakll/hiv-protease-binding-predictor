{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model for HIV-1 Protease Binding Affinity Prediction\n",
    "\n",
    "**Author:** [Your Name]  \n",
    "**Date:** January 2026  \n",
    "**Project:** Predicting protein-ligand binding affinity using molecular properties\n",
    "\n",
    "## Objective\n",
    "Develop machine learning models to predict binding affinity (ΔG) of ligands to HIV-1 protease (1HVR) based solely on ligand molecular properties, eliminating the need for computationally expensive molecular docking simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('data.csv', encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "# Features: Ligand molecular properties only (MW, LogP, HBD, HBA)\n",
    "# Target: Binding affinity (ΔG)\n",
    "\n",
    "feature_cols = ['MW', 'LogP', 'HBD', 'HBA']\n",
    "target_col = 'G'\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Target (y): {y.shape}\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% training, 20% testing\n",
    "# Set random_state for reproducibility\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTrain/Test ratio: {X_train.shape[0]/X_test.shape[0]:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling\n",
    "\n",
    "Standardize features to have mean=0 and std=1. This is critical because features have different scales (MW: 100-900, HBD: 0-20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler and fit on training data only\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for clarity\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols)\n",
    "\n",
    "print(\"Scaled training features (first 5 rows):\")\n",
    "print(X_train_scaled.head())\n",
    "print(f\"\\nMean of scaled features: {X_train_scaled.mean().values}\")\n",
    "print(f\"Std of scaled features: {X_train_scaled.std().values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation\n",
    "\n",
    "We'll train and compare three models:\n",
    "1. **Linear Regression**: Simple baseline\n",
    "2. **Random Forest**: Ensemble method, handles non-linearity\n",
    "3. **Gradient Boosting**: Advanced ensemble method\n",
    "\n",
    "Note: Using Gradient Boosting instead of XGBoost due to package availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=5),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=3)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Cross-validation score (5-fold)\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, \n",
    "                                 cv=5, scoring='r2')\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'cv_r2_mean': cv_scores.mean(),\n",
    "        'cv_r2_std': cv_scores.std(),\n",
    "        'y_test_pred': y_test_pred\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Training R²:   {train_r2:.4f}\")\n",
    "    print(f\"Test R²:       {test_r2:.4f}\")\n",
    "    print(f\"Test RMSE:     {test_rmse:.4f} kcal/mol\")\n",
    "    print(f\"Test MAE:      {test_mae:.4f} kcal/mol\")\n",
    "    print(f\"CV R² (5-fold): {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Train R²': [results[m]['train_r2'] for m in results.keys()],\n",
    "    'Test R²': [results[m]['test_r2'] for m in results.keys()],\n",
    "    'RMSE': [results[m]['test_rmse'] for m in results.keys()],\n",
    "    'MAE': [results[m]['test_mae'] for m in results.keys()],\n",
    "    'CV R²': [results[m]['cv_r2_mean'] for m in results.keys()]\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.round(4)\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = comparison_df.loc[comparison_df['Test R²'].idxmax(), 'Model']\n",
    "print(f\"\\n✓ Best Model: {best_model_name} (Test R² = {comparison_df['Test R²'].max():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# R² comparison\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, comparison_df['Train R²'], width, label='Train R²', alpha=0.8)\n",
    "axes[0].bar(x + width/2, comparison_df['Test R²'], width, label='Test R²', alpha=0.8)\n",
    "axes[0].set_xlabel('Model', fontsize=11)\n",
    "axes[0].set_ylabel('R² Score', fontsize=11)\n",
    "axes[0].set_title('Model Performance: R² Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(comparison_df['Model'], rotation=15, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# RMSE and MAE comparison\n",
    "axes[1].bar(x - width/2, comparison_df['RMSE'], width, label='RMSE', alpha=0.8)\n",
    "axes[1].bar(x + width/2, comparison_df['MAE'], width, label='MAE', alpha=0.8)\n",
    "axes[1].set_xlabel('Model', fontsize=11)\n",
    "axes[1].set_ylabel('Error (kcal/mol)', fontsize=11)\n",
    "axes[1].set_title('Model Performance: Error Metrics', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(comparison_df['Model'], rotation=15, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved as: model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model = results[best_model_name]['model']\n",
    "y_test_pred_best = results[best_model_name]['y_test_pred']\n",
    "\n",
    "print(f\"Analyzing best model: {best_model_name}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Predicted vs Actual\n",
    "axes[0].scatter(y_test, y_test_pred_best, alpha=0.7, s=100, edgecolors='black')\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual ΔG (kcal/mol)', fontsize=11)\n",
    "axes[0].set_ylabel('Predicted ΔG (kcal/mol)', fontsize=11)\n",
    "axes[0].set_title(f'{best_model_name}: Predicted vs Actual', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Add R² annotation\n",
    "r2 = results[best_model_name]['test_r2']\n",
    "axes[0].text(0.05, 0.95, f'R² = {r2:.4f}', transform=axes[0].transAxes,\n",
    "             fontsize=12, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "# Plot 2: Residuals\n",
    "residuals = y_test - y_test_pred_best\n",
    "axes[1].scatter(y_test_pred_best, residuals, alpha=0.7, s=100, edgecolors='black')\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted ΔG (kcal/mol)', fontsize=11)\n",
    "axes[1].set_ylabel('Residuals (kcal/mol)', fontsize=11)\n",
    "axes[1].set_title(f'{best_model_name}: Residual Plot', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('best_model_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved as: best_model_predictions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance:\")\n",
    "    print(feature_importance.to_string(index=False))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance['Feature'], feature_importance['Importance'], \n",
    "             color='steelblue', alpha=0.8, edgecolor='black')\n",
    "    plt.xlabel('Importance', fontsize=11)\n",
    "    plt.ylabel('Feature', fontsize=11)\n",
    "    plt.title(f'{best_model_name}: Feature Importance', fontsize=12, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nFigure saved as: feature_importance.png\")\n",
    "    \n",
    "elif best_model_name == 'Linear Regression':\n",
    "    # For linear regression, show coefficients\n",
    "    coefficients = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Coefficient': best_model.coef_\n",
    "    }).sort_values('Coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    print(\"\\nLinear Regression Coefficients:\")\n",
    "    print(coefficients.to_string(index=False))\n",
    "    \n",
    "    # Plot coefficients\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['green' if x > 0 else 'red' for x in coefficients['Coefficient']]\n",
    "    plt.barh(coefficients['Feature'], coefficients['Coefficient'], \n",
    "             color=colors, alpha=0.8, edgecolor='black')\n",
    "    plt.xlabel('Coefficient Value', fontsize=11)\n",
    "    plt.ylabel('Feature', fontsize=11)\n",
    "    plt.title('Linear Regression: Feature Coefficients', fontsize=12, fontweight='bold')\n",
    "    plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_coefficients.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nFigure saved as: feature_coefficients.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PROJECT SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset: {len(df)} ligands docked to HIV-1 protease (1HVR)\")\n",
    "print(f\"Features: {', '.join(feature_cols)}\")\n",
    "print(f\"Target: Binding Affinity (ΔG)\")\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Test R²: {results[best_model_name]['test_r2']:.4f}\")\n",
    "print(f\"Test RMSE: {results[best_model_name]['test_rmse']:.4f} kcal/mol\")\n",
    "print(f\"Test MAE: {results[best_model_name]['test_mae']:.4f} kcal/mol\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. Molecular properties alone can predict binding affinity with good accuracy\")\n",
    "print(\"2. Model eliminates need for computationally expensive docking simulations\")\n",
    "print(\"3. Can be used for rapid screening of large ligand libraries\")\n",
    "print(\"4. Molecular Weight (MW) is the strongest predictor of binding affinity\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FUTURE WORK\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. Expand dataset with more diverse ligands\")\n",
    "print(\"2. Include multiple protein targets for generalization\")\n",
    "print(\"3. Explore deep learning approaches (neural networks)\")\n",
    "print(\"4. Validate predictions with experimental binding data\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
